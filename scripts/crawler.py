import os
import re
import time
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

# --- é…ç½®åŒºåŸŸ ---
TARGET_URL = "https://www.12306.cn/index/"
# æ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œé˜²æ­¢è¢«åçˆ¬è™«æ‹¦æˆª
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}
# ä¸‹è½½ä¿å­˜è·¯å¾„ (ä¸´æ—¶æ–‡ä»¶å¤¹ï¼Œæ•´ç†å¥½åå†æ”¾å…¥ frontend)
SAVE_DIR = "downloaded_assets"


class ResourceCrawler:
    def __init__(self, base_url, save_dir):
        self.base_url = base_url
        self.save_dir = save_dir
        self.visited_urls = set()
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

        # åˆ›å»ºä¿å­˜ç›®å½•
        self.img_dir = os.path.join(save_dir, "images")
        self.css_dir = os.path.join(save_dir, "css")
        os.makedirs(self.img_dir, exist_ok=True)
        os.makedirs(self.css_dir, exist_ok=True)

    def download_file(self, url, category="images"):
        """é€šç”¨æ–‡ä»¶ä¸‹è½½å‡½æ•°"""
        if url in self.visited_urls:
            return None

        try:
            # 1. å¤„ç† URL
            if url.startswith("//"):
                url = "https:" + url
            full_url = urljoin(self.base_url, url)

            # æ’é™¤éå›¾ç‰‡/CSSèµ„æº
            if not any(
                full_url.lower().endswith(ext)
                for ext in [".png", ".jpg", ".jpeg", ".gif", ".svg", ".css"]
            ):
                return None

            # 2. å‘é€è¯·æ±‚
            print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½: {full_url} ...")
            response = self.session.get(full_url, timeout=10)

            if response.status_code == 200:
                # 3. æå–æ–‡ä»¶å
                parsed_url = urlparse(full_url)
                filename = os.path.basename(parsed_url.path)
                if not filename:
                    filename = f"resource_{int(time.time())}.ext"

                # 4. ä¿å­˜æ–‡ä»¶
                target_dir = self.css_dir if category == "css" else self.img_dir
                save_path = os.path.join(target_dir, filename)

                with open(save_path, "wb") as f:
                    f.write(response.content)

                self.visited_urls.add(url)
                print(f"âœ… å·²ä¿å­˜: {save_path}")
                return save_path, response.text  # è¿”å›å†…å®¹ä¾›CSSåˆ†æç”¨
            else:
                print(f"âŒ ä¸‹è½½å¤±è´¥ (çŠ¶æ€ç  {response.status_code}): {full_url}")

        except Exception as e:
            print(f"âš ï¸ ä¸‹è½½å‡ºé”™: {url} - {e}")
        return None, None

    def parse_css_for_images(self, css_content, css_url):
        """ä» CSS å†…å®¹ä¸­æå– url(...) é‡Œçš„å›¾ç‰‡"""
        if not css_content:
            return

        # æ­£åˆ™åŒ¹é… url('...') æˆ– url("...") æˆ– url(...)
        # è¿™æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„æ­£åˆ™ï¼Œèƒ½æ•è·å¤§éƒ¨åˆ† CSS å›¾ç‰‡å¼•ç”¨
        urls = re.findall(r'url\((?:[\'"]?)(.*?)(?:[\'"]?)\)', css_content)

        for relative_url in urls:
            # è¿‡æ»¤æ‰ base64 æ•°æ®å’Œæ— å…³é“¾æ¥
            if relative_url.startswith("data:") or len(relative_url) < 5:
                continue

            # æ³¨æ„ï¼šCSS é‡Œçš„å›¾ç‰‡è·¯å¾„æ˜¯ç›¸å¯¹äº CSS æ–‡ä»¶æœ¬èº«çš„ï¼
            # æ‰€ä»¥æˆ‘ä»¬è¦ç”¨ css_url ä½œä¸º base æ¥æ‹¼æ¥
            absolute_img_url = urljoin(css_url, relative_url)
            self.download_file(absolute_img_url, category="images")

    def run(self):
        print(f"ğŸš€ å¼€å§‹æŠ“å–: {self.base_url}")

        try:
            # 1. è®¿é—®ä¸»é¡µ
            response = self.session.get(self.base_url)
            response.encoding = "utf-8"
            soup = BeautifulSoup(response.text, "html.parser")

            # 2. æŠ“å– HTML ä¸­çš„ img æ ‡ç­¾
            print("\n--- ğŸ“· åˆ†æ HTML å›¾ç‰‡ ---")
            images = soup.find_all("img")
            for img in images:
                src = img.get("src")
                if src:
                    self.download_file(src, category="images")

            # 3. æŠ“å– CSS æ–‡ä»¶ï¼Œå¹¶æ·±å…¥åˆ†æ CSS é‡Œçš„èƒŒæ™¯å›¾
            print("\n--- ğŸ¨ åˆ†æ CSS åŠèƒŒæ™¯å›¾ ---")
            links = soup.find_all("link", rel="stylesheet")
            for link in links:
                href = link.get("href")
                if href:
                    # ä¸‹è½½ CSS æ–‡ä»¶æœ¬èº«ï¼Œå¹¶è·å–å…¶å†…å®¹è¿›è¡ŒäºŒæ¬¡åˆ†æ
                    saved_path, css_content = self.download_file(href, category="css")

                    # å¦‚æœ CSS ä¸‹è½½æˆåŠŸï¼Œè§£æé‡Œé¢çš„å›¾ç‰‡
                    if saved_path:
                        # æ„é€ è¿™ä¸ª CSS æ–‡ä»¶çš„å®Œæ•´ URLï¼Œç”¨äºè§£æå®ƒå†…éƒ¨çš„ç›¸å¯¹è·¯å¾„
                        full_css_url = urljoin(self.base_url, href)
                        self.parse_css_for_images(css_content, full_css_url)

        except Exception as e:
            print(f"âŒ å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

        print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼èµ„æºå·²ä¿å­˜åœ¨: {self.save_dir}")


if __name__ == "__main__":
    crawler = ResourceCrawler(TARGET_URL, SAVE_DIR)
    crawler.run()
